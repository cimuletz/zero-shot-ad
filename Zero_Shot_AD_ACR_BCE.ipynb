{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpJ1BRjqsFpOBDXEjU53bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cimuletz/zero-shot-ad/blob/main/Zero_Shot_AD_ACR_BCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "_yNC84cbR4b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z_zytTGMP0T"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from typing import Iterator, List, Callable, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import sys\n",
        "torch.manual_seed(115)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACR-BCE"
      ],
      "metadata": {
        "id": "6zDXlwsVs2M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maml_init_(module):\n",
        "    torch.nn.init.xavier_uniform_(module.weight.data, gain=1.0)\n",
        "    torch.nn.init.constant_(module.bias.data, 0.0)\n",
        "    return module\n"
      ],
      "metadata": {
        "id": "3M9DBeVz6Igu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maybe use affine?\n",
        "class BasicBlock(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels, \n",
        "               out_channels,\n",
        "               kernel_size,\n",
        "               stride,\n",
        "               max_pool=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_pool = max_pool\n",
        "    self.max_pool_layer = torch.nn.MaxPool2d(kernel_size, stride)\n",
        "    self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "    self.activation_fn = nn.ReLU()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=0, bias=True)\n",
        "\n",
        "    #maml initialization of the weights\n",
        "    maml_init_(self.conv)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = self.batch_norm(x)\n",
        "      x = self.activation_fn(x)\n",
        "      if self.max_pool == True:\n",
        "        x = self.max_pool_layer(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "YgHb20j2H6jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOpjSE1OV_zB"
      },
      "outputs": [],
      "source": [
        "class ACRBCE(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        #batch_sz x 28 x 28\n",
        "        self.conv1 = BasicBlock(1, 64, 3, 1)\n",
        "        #batch_sz x 24 x 24\n",
        "        self.conv2 = BasicBlock(64, 64, 3, 1)\n",
        "        #batch_sz x 20 x 20\n",
        "        self.conv3 = BasicBlock(64, 64, 3, 1)\n",
        "        #batch_sz x 16 x 16\n",
        "        self.conv4 = BasicBlock(64, 64, 3, 1)\n",
        "        #batch_sz x 12 x 12\n",
        "        self.fc1 = nn.Linear(in_features = 64 * 12 * 12, out_features = 1)\n",
        "\n",
        "        # self.fc1.weight.data.normal_()\n",
        "        # self.fc1.bias.data.mul_(0.0)\n",
        "\n",
        "        self.activation_fn = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "\n",
        "        return x "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "4Ydy5E7IxrXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz"
      ],
      "metadata": {
        "id": "Wk6fPxA7Blxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "p55P6ZRKHLyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Pj distributions, j=0...5\n",
        "\n",
        "# Full MNIST dataset\n",
        "mnist = datasets.MNIST('./', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "\n",
        "mnist_test = datasets.MNIST('./', train=False, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "\n",
        "# Dataset split by class\n",
        "\n",
        "train_data = mnist.data.float() / 255\n",
        "train_targets = mnist.targets\n",
        "\n",
        "test_data = mnist_test.data.float() / 255\n",
        "test_targets = mnist_test.targets\n",
        "\n",
        "datasets_by_class = [[(train_data[idx], i) for idx in range(len(train_data)) if train_targets[idx] == i] for i in range(10)]\n",
        "datasets_by_class_test = [[(test_data[idx], i) for idx in range(len(test_data)) if test_targets[idx] == i] for i in range(10)]"
      ],
      "metadata": {
        "id": "jOG9NUolBpjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets_by_class[1][0][0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E0N4zjSJqMZ",
        "outputId": "329af429-6c56-43a7-bb52-813c721aca2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this function returns a dataloader (or two: <training/validation>, if training mode is set True)\n",
        "# which loads data having a certain label, mixed \n",
        "# with data having different labels, the anomalous data being \n",
        "# in the given ratio\n",
        "\n",
        "import random\n",
        "import copy\n",
        "\n",
        "def get_training_data(training, label, anomaly_ratio, batch_size):\n",
        "  if training:\n",
        "    dataset = copy.deepcopy(datasets_by_class[label])\n",
        "  else:\n",
        "    dataset = copy.deepcopy(datasets_by_class_test[label])\n",
        "\n",
        "  for i in range(len(dataset)):\n",
        "    dataset[i] = (dataset[i][0], torch.tensor([0], dtype=torch.float32))\n",
        "\n",
        "  anomaly_target_count = int(len(dataset) * anomaly_ratio)\n",
        "\n",
        "  max_anomaly_class = 9\n",
        "\n",
        "  for i in range(anomaly_target_count):\n",
        "    target_class = random.randint(6, max_anomaly_class)\n",
        "\n",
        "    # randomly picking anomaly label, must be different from given label\n",
        "    while target_class == label:\n",
        "      target_class = random.randint(6, max_anomaly_class)\n",
        "\n",
        "    # picking a random example from there\n",
        "\n",
        "    if training:\n",
        "      random_index = random.randint(0, len(datasets_by_class[target_class]) - 1)\n",
        "      random_example = datasets_by_class[target_class][random_index]\n",
        "    else:\n",
        "      random_index = random.randint(0, len(datasets_by_class_test[target_class]) - 1)\n",
        "      random_example = datasets_by_class_test[target_class][random_index]\n",
        "\n",
        "    random_example = (random_example[0], torch.tensor([1], dtype=torch.float32))\n",
        "    dataset.append(random_example)\n",
        "  \n",
        "  random.shuffle(dataset)\n",
        "\n",
        "  if training == 2:\n",
        "    return dataset\n",
        "\n",
        "  if training:\n",
        "    # returning two dataloaders if train flag is set True\n",
        "    split_index = int(len(dataset) / 5)\n",
        "\n",
        "    test_dataset = dataset[:split_index]\n",
        "    training_dataset = dataset[split_index:]\n",
        "\n",
        "    training_dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      shuffle=True, \n",
        "                                                      drop_last=True)\n",
        "    \n",
        "    testing_dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      shuffle=True,\n",
        "                                                      drop_last=True)\n",
        "\n",
        "    return training_dataloader, testing_dataloader\n",
        "  else:  \n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "YZJrVYW8zhpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "rwYm8UgnDx2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ACRTrainer:\n",
        "  def __init__(self, model, loss_fn, no_classes, max_iter = 100):\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    self.model = model.to(self.device)\n",
        "    self.loss_fn = loss_fn\n",
        "    self.max_iter = max_iter\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "    #self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9)\n",
        "    self.no_classes = no_classes\n",
        "\n",
        "  # singular step in training\n",
        "  def train_step_distributions(self, loaders):\n",
        "    self.model.train()\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    total_exp = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "\n",
        "    # one epoch passing through all the datasets\n",
        "    for j in range(0, self.no_classes):\n",
        "        loader = loaders[j][0]\n",
        "        i = 0\n",
        "        loss = 0\n",
        "        for _, (x_train, y_train) in enumerate(loader):\n",
        "          exp_num = len(x_train)\n",
        "          total_exp += exp_num\n",
        "          i += 1\n",
        "\n",
        "          x_train = x_train.view(x_train.shape[0], 1, 28, 28)\n",
        "          x = x_train.to(self.device)\n",
        "          y = y_train.to(self.device)\n",
        "          out = self.model(x)\n",
        "          loss += self.loss_fn(out, y)\n",
        "\n",
        "          # do backprop every 100 batches, otherwise running out of memory\n",
        "          if i == 100:\n",
        "              total_loss += loss.item()\n",
        "              loss /= 100\n",
        "              self.optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              i = 0\n",
        "              loss = 0\n",
        "        if i > 0:\n",
        "          total_loss += loss.item()\n",
        "          loss /= i\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()  \n",
        "          losses.append(loss.item())\n",
        "          total_loss += loss.item()\n",
        "\n",
        "    return total_loss/total_exp, losses\n",
        "\n",
        "  #validation/test function which calculates auc and loss\n",
        "  def outlier_score(self, loader, mode='val'):\n",
        "    auc = []\n",
        "    loss = 0\n",
        "    num_examples = 0\n",
        "    pred = []\n",
        "    truth = []\n",
        "    i = 0\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      for _, (x_train, y_train) in enumerate(loader):\n",
        "        num_examples += len(x_train)\n",
        "        x_train = x_train.view(x_train.shape[0], 1, 28, 28)\n",
        "        x = x_train.to(self.device)\n",
        "        y = y_train.to(self.device)\n",
        "\n",
        "        out = self.model(x)\n",
        "        aux_loss = self.loss_fn(out, y)\n",
        "        loss += aux_loss.item()\n",
        "        pred.extend(np.array(out.cpu()))\n",
        "        truth.extend(y_train.cpu().numpy())\n",
        "        auc_i = roc_auc_score(y_train.cpu().numpy(), np.array(out.cpu()))\n",
        "        auc.append(auc_i)\n",
        "    return loss/num_examples, roc_auc_score(truth, pred)\n",
        "\n",
        "  # starts the training routine\n",
        "  def train_distributions(self, loaders):\n",
        "    val_auc, val_f1 = -1, -1\n",
        "    test_auc, test_f1, test_score = None, None,None\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    for i in range(0, self.max_iter):\n",
        "      train_loss, losses1 = self.train_step_distributions(loaders)\n",
        "      losses.append(train_loss)\n",
        "      if i % 1 == 0:\n",
        "        total_val_loss = 0\n",
        "        for j in range(0, self.no_classes):\n",
        "          mode = 0\n",
        "          if j == 1:\n",
        "            mode = 'ones'\n",
        "          else:\n",
        "            mode = 'val'\n",
        "          val_loader = loaders[j][1]\n",
        "          val_loss, val_auc = self.outlier_score(val_loader, mode)\n",
        "          print(f\"Iteration: {i}, Class:{j},  TL: {train_loss}, VL:{val_loss}, VA:{val_auc}\")\n",
        "          total_val_loss += val_loss\n",
        "        val_losses.append(total_val_loss/self.no_classes)\n",
        "      print()\n",
        "    return losses, val_losses\n",
        "\n",
        "  # starts the testing routine\n",
        "  def test(self, loader):\n",
        "    test_score, test_auc = 0, 0\n",
        "    self.model.eval()\n",
        "    test_score, test_auc = self.outlier_score(loader, 'test')\n",
        "    return test_score, test_auc\n",
        "\n"
      ],
      "metadata": {
        "id": "JLVtjyCnDxGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run ACR-BCE"
      ],
      "metadata": {
        "id": "30VHpHrz0S_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ratios of anomalous examples to normal examples\n",
        "test_pi = 0.1 \n",
        "train_pi = 0.8\n",
        "\n",
        "# batch sizes\n",
        "test_sz = 64\n",
        "train_sz = 64\n",
        "\n",
        "# training iterations and classes used\n",
        "no_classes_training = 5\n",
        "no_iterations = 10"
      ],
      "metadata": {
        "id": "GYZ2vfmaFZ-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = get_training_data(0, 5, test_pi, test_sz)\n",
        "\n",
        "# initialize training dataloaders\n",
        "loaders = []\n",
        "for i in range(0, no_classes_training):\n",
        "  train_loader, val_loader = get_training_data(1, i, train_pi, train_sz)\n",
        "  loaders.append((train_loader, val_loader))\n",
        "test_loader = iter(test_loader)\n",
        "\n",
        "# train model\n",
        "model = ACRBCE()\n",
        "loss = nn.BCELoss()\n",
        "trainer = ACRTrainer(model, loss, no_classes_training, no_iterations)\n",
        "\n",
        "# calculate test auc\n",
        "losses, val_losses = trainer.train_distributions(loaders)\n",
        "test_score, test_auc = trainer.test(test_loader)\n",
        "print(f'Test score: {test_score}, Test AUC: {test_auc}')"
      ],
      "metadata": {
        "id": "m68Izj-60VfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(range(0,len(losses)), losses, 'g', label='Training loss')\n",
        "plt.plot(range(0,len(losses)), val_losses, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9dp1S0swrQtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance between datasets using MMD"
      ],
      "metadata": {
        "id": "-mjTWJv9P59G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MMD(x, y, kernel):\n",
        "    \"\"\"Emprical maximum mean discrepancy. The lower the result\n",
        "       the more evidence that distributions are the same.\n",
        "\n",
        "    Args:\n",
        "        x: first sample, distribution P\n",
        "        y: second sample, distribution Q\n",
        "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
        "    \"\"\"\n",
        "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
        "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
        "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
        "    \n",
        "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
        "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
        "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
        "    \n",
        "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
        "                  torch.zeros(xx.shape).to(device),\n",
        "                  torch.zeros(xx.shape).to(device))\n",
        "    \n",
        "    if kernel == \"multiscale\":\n",
        "        \n",
        "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
        "        for a in bandwidth_range:\n",
        "            XX += a**2 * (a**2 + dxx)**-1\n",
        "            YY += a**2 * (a**2 + dyy)**-1\n",
        "            XY += a**2 * (a**2 + dxy)**-1\n",
        "            \n",
        "    if kernel == \"rbf\":\n",
        "      \n",
        "        bandwidth_range = [10, 15, 20, 50]\n",
        "        for a in bandwidth_range:\n",
        "            XX += torch.exp(-0.5*dxx/a)\n",
        "            YY += torch.exp(-0.5*dyy/a)\n",
        "            XY += torch.exp(-0.5*dxy/a)\n",
        "      \n",
        "      \n",
        "\n",
        "    return torch.mean(XX + YY - 2. * XY)"
      ],
      "metadata": {
        "id": "e9BoZIqxP8__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (i,j) in [(0, 1.0), (0, 0.5), (0, 0.1)]:\n",
        "  for (k, l) in [(5, 1.0), (5, 0.5), (5, 0.1)]:\n",
        "    dataset1 = torch.cat([train for train, target in (get_training_data(2, i, j, 1)[:400])]).to(device)\n",
        "    dataset2 = torch.cat([train for train, target in (get_training_data(2, k, l, 1)[:400])]).to(device)\n",
        "    res = (MMD(dataset1, dataset2, 'multiscale'))\n",
        "    print(f'Dataset {i}, ratio {j}; dataset {k}, ratio {l} result: {res}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI562Q3oQArF",
        "outputId": "a884eac1-c777-4186-f656-f38fdca9b7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 0, ratio 1.0; dataset 5, ratio 1.0 result: 0.01399888563901186\n",
            "Dataset 0, ratio 1.0; dataset 5, ratio 0.5 result: 0.015456795692443848\n",
            "Dataset 0, ratio 1.0; dataset 5, ratio 0.1 result: 0.01581188477575779\n",
            "Dataset 0, ratio 0.5; dataset 5, ratio 1.0 result: 0.016917800530791283\n",
            "Dataset 0, ratio 0.5; dataset 5, ratio 0.5 result: 0.019765865057706833\n",
            "Dataset 0, ratio 0.5; dataset 5, ratio 0.1 result: 0.020520687103271484\n",
            "Dataset 0, ratio 0.1; dataset 5, ratio 1.0 result: 0.03980853781104088\n",
            "Dataset 0, ratio 0.1; dataset 5, ratio 0.5 result: 0.03733474761247635\n",
            "Dataset 0, ratio 0.1; dataset 5, ratio 0.1 result: 0.04190833494067192\n"
          ]
        }
      ]
    }
  ]
}